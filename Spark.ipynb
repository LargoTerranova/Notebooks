{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Localhost\n",
    "# http://localhost:4040/ \n",
    "\n",
    "# Packages\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, lit, col\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import rank\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.types import StructType,StructField \n",
    "from pyspark.sql.types import StringType, IntegerType, ArrayType, DateType\n",
    "from pyspark.sql.types import StringType, ArrayType,StructType,StructField\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "# F.concat()\n",
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)]\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "df_concat = spark.createDataFrame(data=data, schema = columns)\n",
    "\n",
    "\n",
    "# F.explode()\n",
    "spark = SparkSession.builder.appName('pyspark-by-examples').getOrCreate()\n",
    "arrayData = [\n",
    "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "        ('Washington',None,None),\n",
    "        ('Jefferson',['1','2'],{})]\n",
    "df_explode = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\n",
    "\n",
    "\n",
    "# when()\n",
    "data = [(\"James\",\"M\",60000),(\"Michael\",\"M\",70000),\n",
    "        (\"Robert\",None,400000),(\"Maria\",\"F\",500000),\n",
    "        (\"Jen\",\"\",None)]\n",
    "columns = [\"name\",\"gender\",\"salary\"]\n",
    "df_when = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "\n",
    "# split()\n",
    "data=data = [('James','','Smith','1991-04-01'),\n",
    "  ('Michael','Rose','','2000-05-19'),\n",
    "  ('Robert','','Williams','1978-09-05'),\n",
    "  ('Maria','Anne','Jones','1967-12-01'),\n",
    "  ('Jen','Mary','Brown','1980-02-17')]\n",
    "columns=[\"firstname\",\"middlename\",\"lastname\",\"dob\"]\n",
    "df_split = spark.createDataFrame(data,columns)\n",
    "\n",
    "\n",
    "# rank()\n",
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100))\n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df_rank = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "\n",
    "\n",
    "# withColumn()\n",
    "data = [('James','','Smith','2020-01-01','M',3000),\n",
    "  ('Michael','Rose','','2021-01-01','M',4000),\n",
    "  ('Robert','','Williams','2021-01-01','M',4000),\n",
    "  ('Maria','Anne','Jones','2020-01-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1982-01-27','F',-1)]\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "df_wc = spark.createDataFrame(data=data, schema = columns)\n",
    "\n",
    "\n",
    "# collect_list()\n",
    "simpleData = [(\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)]\n",
    "schema = [\"employee_name\", \"department\", \"salary\"]\n",
    "df_collectlist = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "\n",
    "\n",
    "# df_filter\n",
    "data = [\n",
    "    ((\"James\",\"\",\"Smith\"),[\"Java\",\"Scala\",\"C++\"],\"OH\",\"M\",\"Male\"),\n",
    "    ((\"Anna\",\"Rose\",\"\"),[\"Spark\",\"Java\",\"C++\"],\"NY\",\"F\",\"Female\"),\n",
    "    ((\"Julia\",\"\",\"Williams\"),[\"CSharp\",\"VB\"],\"OH\",\"F\",\"Male\"),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),[\"CSharp\",\"VB\"],\"NY\",\"M\",\"Female\"),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),[\"CSharp\",\"VB\"],\"NY\",\"M\",\"Male\"),\n",
    "    ((\"Mike\",\"Mary\",\"Williams\"),[\"Python\",\"VB\"],\"OH\",\"M\",\"Female\")\n",
    " ]\n",
    "        \n",
    "schema = StructType([\n",
    "     StructField('name', StructType([\n",
    "        StructField('firstname', StringType(), True),\n",
    "        StructField('middlename', StringType(), True),\n",
    "         StructField('lastname', StringType(), True)\n",
    "     ])),\n",
    "     StructField('languages', ArrayType(StringType()), True),\n",
    "     StructField('state', StringType(), True),\n",
    "     StructField('gender', StringType(), True),\n",
    "     StructField('gender2', StringType(), True)\n",
    " ])\n",
    "\n",
    "df_filter = spark.createDataFrame(data = data, schema = schema)\n",
    "\n",
    "\n",
    "# array()\n",
    "data = [\n",
    " (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"],\"OH\",\"CA\"),\n",
    " (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"],\"NY\",\"NJ\"),\n",
    " (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"],\"UT\",\"NV\")]\n",
    "schema = StructType([ \n",
    "    StructField(\"name\",StringType(),True), \n",
    "    StructField(\"languagesAtSchool\",ArrayType(StringType()),True), \n",
    "    StructField(\"languagesAtWork\",ArrayType(StringType()),True), \n",
    "    StructField(\"currentState\", StringType(), True), \n",
    "    StructField(\"previousState\", StringType(), True)])\n",
    "\n",
    "df_array = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# alert()\n",
    "data = [('101', \"MKD\", \"2021-01-01 00:00:00.0\"),\n",
    "        ('102', \"MKD\", \"2021-01-01 00:00:00.0\"),\n",
    "        ('103', \"MKD\", \"2020-01-01 00:00:00.0\"),\n",
    "        ('104', \"MKD\", \"2020-01-01 00:00:00.0\"),\n",
    "        ('105', \"MKD\", \"2019-01-01 00:00:00.0\"),\n",
    "        ('106', \"MKD\", \"2019-01-01 00:00:00.0\"),\n",
    "        ('201', \"OMA\", \"2021-01-01 00:00:00.0\"),\n",
    "        ('202', \"OMA\", \"2021-01-01 00:00:00.0\"),\n",
    "        ('203', \"OMA\", \"2020-01-01 00:00:00.0\"),\n",
    "        ('204', \"OMA\", \"2020-01-01 00:00:00.0\"),\n",
    "        ('205', \"OMA\", \"2019-01-01 00:00:00.0\"),\n",
    "        ('206', \"OMA\", \"2019-01-01 00:00:00.0\"),\n",
    "        ('301', \"ICE\", \"2021-01-01 00:00:00.0\"),\n",
    "        ('302', \"ICE\", \"2021-01-01 00:00:00.0\"),\n",
    "        ('303', \"ICE\", \"2020-01-01 00:00:00.0\"),\n",
    "        ('304', \"ICE\", \"2020-01-01 00:00:00.0\"),\n",
    "        ('305', \"ICE\", \"2019-01-01 00:00:00.0\"),\n",
    "        ('306', \"ICE\", \"2019-01-01 00:00:00.0\"),\n",
    "        ('401', \"CH\",  \"2021-01-01 00:00:00.0\"),\n",
    "        ('402', \"CH\",  \"2021-01-01 00:00:00.0\"),\n",
    "        ('403', \"CH\",  \"2020-01-01 00:00:00.0\"),\n",
    "        ('404', \"CH\",  \"2020-01-01 00:00:00.0\"),\n",
    "        ('405', \"CH\",  \"2019-01-01 00:00:00.0\"),\n",
    "        ('406', \"CH\",  \"2019-01-01 00:00:00.0\"),\n",
    "        ('501', \"PRT\", \"2021-01-01 00:00:00.0\"),\n",
    "        ('502', \"PRT\", \"2021-01-01 00:00:00.0\"),\n",
    "        ('503', \"PRT\", \"2020-01-01 00:00:00.0\"),\n",
    "        ('504', \"PRT\", \"2020-01-01 00:00:00.0\"),\n",
    "        ('505', \"PRT\", \"2019-01-01 00:00:00.0\"),\n",
    "        ('506', \"PRT\", \"2019-01-01 00:00:00.0\"),\n",
    "]\n",
    "columns = [\"ID\",\"COUNTRY\",\"DATE\",]\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "df_alert = spark.createDataFrame(data=data, schema = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Window.Window\n",
    "#Types.StringType\n",
    "#Transforms.api : transform_df, Input, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+\n",
      "| ID|COUNTRY|                DATE|\n",
      "+---+-------+--------------------+\n",
      "|101|    MKD|2021-01-01 00:00:...|\n",
      "|101|    MKD|2021-01-01 00:00:...|\n",
      "|101|    MKD|2020-01-01 00:00:...|\n",
      "|101|    MKD|2020-01-01 00:00:...|\n",
      "|101|    MKD|2019-01-01 00:00:...|\n",
      "|101|    MKD|2019-01-01 00:00:...|\n",
      "|201|    OMA|2021-01-01 00:00:...|\n",
      "|202|    OMA|2021-01-01 00:00:...|\n",
      "|203|    OMA|2020-01-01 00:00:...|\n",
      "|204|    OMA|2020-01-01 00:00:...|\n",
      "|205|    OMA|2019-01-01 00:00:...|\n",
      "|206|    OMA|2019-01-01 00:00:...|\n",
      "|301|    ICE|2021-01-01 00:00:...|\n",
      "|302|    ICE|2021-01-01 00:00:...|\n",
      "|303|    ICE|2020-01-01 00:00:...|\n",
      "|304|    ICE|2020-01-01 00:00:...|\n",
      "|305|    ICE|2019-01-01 00:00:...|\n",
      "|306|    ICE|2019-01-01 00:00:...|\n",
      "|401|     CH|2021-01-01 00:00:...|\n",
      "|402|     CH|2021-01-01 00:00:...|\n",
      "+---+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+-------+----------+---------+------+--------+\n",
      "| ID|COUNTRY|      DATE|NR_ALERTS|NUMBER|ALERT_ID|\n",
      "+---+-------+----------+---------+------+--------+\n",
      "|101|    MKD|2021-01-01|        2|     1|   101_1|\n",
      "|101|    MKD|2021-01-01|        2|     2|   101_2|\n",
      "|202|    OMA|2021-01-01|        1|     1|   202_1|\n",
      "|401|     CH|2021-01-01|        1|     1|   401_1|\n",
      "|302|    ICE|2021-01-01|        1|     1|   302_1|\n",
      "|502|    PRT|2021-01-01|        1|     1|   502_1|\n",
      "|501|    PRT|2021-01-01|        1|     1|   501_1|\n",
      "|201|    OMA|2021-01-01|        1|     1|   201_1|\n",
      "|402|     CH|2021-01-01|        1|     1|   402_1|\n",
      "|301|    ICE|2021-01-01|        1|     1|   301_1|\n",
      "+---+-------+----------+---------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_alert.show()\n",
    "df = df_alert\n",
    "df = df.withColumn(\"DATE\", df.DATE.cast(DateType()))\n",
    "\n",
    "max_date = df.agg({\"DATE\":\"max\"}).collect()[0][0]\n",
    "df = df.filter(F.col(\"DATE\") == F.lit(max_date))\n",
    "\n",
    "column_list = [\"ID\"]\n",
    "w = Window.partitionBy([col(x) for x in column_list]).orderBy(\"ID\")\n",
    "df = df.withColumn(\"NR_ALERTS\", F.count(\"ID\").over(w))\n",
    "df = df.withColumn(\"NUMBER\", F.row_number().over(w))\n",
    "df = df.withColumn(\"alert_id\", F.concat(df.ID, F.lit(\"_\"), df.NUMBER))\n",
    "\n",
    "df = df.toDF(*[c.upper() for c in df.columns])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+------+\n",
      "| ID|COUNTRY|      DATE|NUMBER|\n",
      "+---+-------+----------+------+\n",
      "|205|    OMA|2019-01-01|     1|\n",
      "|101|    MKD|2021-01-01|     1|\n",
      "|101|    OMA|2021-01-01|     2|\n",
      "|101|    ICE|2021-01-01|     3|\n",
      "|406|     CH|2019-01-01|     1|\n",
      "|203|    OMA|2020-01-01|     1|\n",
      "|202|    OMA|2021-01-01|     1|\n",
      "|503|    PRT|2020-01-01|     1|\n",
      "|401|     CH|2021-01-01|     1|\n",
      "|206|    OMA|2019-01-01|     1|\n",
      "|302|    ICE|2021-01-01|     1|\n",
      "|502|    PRT|2021-01-01|     1|\n",
      "|505|    PRT|2019-01-01|     1|\n",
      "|501|    PRT|2021-01-01|     1|\n",
      "|104|    MKD|2020-01-01|     1|\n",
      "|404|     CH|2020-01-01|     1|\n",
      "|102|    MKD|2021-01-01|     1|\n",
      "|403|     CH|2020-01-01|     1|\n",
      "|103|    MKD|2020-01-01|     1|\n",
      "|506|    PRT|2019-01-01|     1|\n",
      "+---+-------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# F.row_number()\n",
    "# Window function: returns a sequential number starting at 1 within a window partition.\n",
    "# If inside a window partion there are records with the same values, then they number like 1,2,3,...\n",
    "# If two unique records do not exist, then it number like 1,1,1,...\n",
    "df = df_alert\n",
    "df = df.withColumn(\"DATE\", df.DATE.cast(DateType()))\n",
    "\n",
    "column_list = [\"ID\"]\n",
    "w = Window.partitionBy([col(x) for x in column_list]).orderBy(\"ID\")\n",
    "df = df.withColumn(\"NUMBER\", F.row_number().over(w))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+\n",
      "|EmpId|Salary|lit_value1|\n",
      "+-----+------+----------+\n",
      "|111  |50000 |1         |\n",
      "|222  |60000 |1         |\n",
      "|333  |40000 |1         |\n",
      "+-----+------+----------+\n",
      "\n",
      "+-----+------+----------+\n",
      "|EmpId|Salary|new_column|\n",
      "+-----+------+----------+\n",
      "|111  |50000 |50000     |\n",
      "|222  |60000 |60000     |\n",
      "|333  |40000 |40000     |\n",
      "+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# F.lit()\n",
    "# lit() and typedLit() are used to add a new column to DataFrame by assigning a literal or constant value. Both these functions return Column type as return type. \n",
    "data = [(\"111\", 50000),(\"222\", 60000),(\"333\", 40000)]\n",
    "columns= [\"EmpId\", \"Salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "# Example 1\n",
    "df2 = df.select(col(\"EmpId\"), col(\"Salary\"), lit(\"1\").alias(\"lit_value1\"))\n",
    "df2.show(truncate=False)\n",
    "\n",
    "dfx = df.withColumn(\"new_column\", col(\"Salary\"))\n",
    "dfx.show(truncate=False)\n",
    "\n",
    "# Example 2\n",
    "#df3 = df2.withColumn(\"lit_value2\", when(col(\"Salary\") >=40000 & col(\"Salary\") <= 50000, lit(\"100\")).otherwise(lit(\"200\")))\n",
    "#df3.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n",
      "+--------------+----------+------+------+\n",
      "|FullName      |dob       |gender|salary|\n",
      "+--------------+----------+------+------+\n",
      "|JamesSmith    |1991-04-01|M     |3000  |\n",
      "|MichaelRose   |2000-05-19|M     |4000  |\n",
      "|RobertWilliams|1978-09-05|M     |4000  |\n",
      "|MariaAnneJones|1967-12-01|F     |4000  |\n",
      "|JenMaryBrown  |1980-02-17|F     |-1    |\n",
      "+--------------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# F.concat()\n",
    "# To concatenate (combine) multiple columns into a single column\n",
    "df_concat.show()\n",
    "df2 = df_concat.select(F.concat(df_concat.firstname, df_concat.middlename, df_concat.lastname).alias(\"FullName\"),\"dob\",\"gender\",\"salary\")\n",
    "df2.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- knownLanguages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+----------+-------------------+--------------------+\n",
      "|      name|     knownLanguages|          properties|\n",
      "+----------+-------------------+--------------------+\n",
      "|     James|      [Java, Scala]|{eye -> brown, ha...|\n",
      "|   Michael|[Spark, Java, null]|{eye -> null, hai...|\n",
      "|    Robert|         [CSharp, ]|{eye -> , hair ->...|\n",
      "|Washington|               null|                null|\n",
      "| Jefferson|             [1, 2]|                  {}|\n",
      "+----------+-------------------+--------------------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- col: string (nullable = true)\n",
      "\n",
      "+---------+------+\n",
      "|     name|   col|\n",
      "+---------+------+\n",
      "|    James|  Java|\n",
      "|    James| Scala|\n",
      "|  Michael| Spark|\n",
      "|  Michael|  Java|\n",
      "|  Michael|  null|\n",
      "|   Robert|CSharp|\n",
      "|   Robert|      |\n",
      "|Jefferson|     1|\n",
      "|Jefferson|     2|\n",
      "+---------+------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- key: string (nullable = false)\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+-------+----+-----+\n",
      "|   name| key|value|\n",
      "+-------+----+-----+\n",
      "|  James| eye|brown|\n",
      "|  James|hair|black|\n",
      "|Michael| eye| null|\n",
      "|Michael|hair|brown|\n",
      "| Robert| eye|     |\n",
      "| Robert|hair|  red|\n",
      "+-------+----+-----+\n",
      "\n",
      "F.explode_outer() with array\n",
      "+----------+------+\n",
      "|      name|   col|\n",
      "+----------+------+\n",
      "|     James|  Java|\n",
      "|     James| Scala|\n",
      "|   Michael| Spark|\n",
      "|   Michael|  Java|\n",
      "|   Michael|  null|\n",
      "|    Robert|CSharp|\n",
      "|    Robert|      |\n",
      "|Washington|  null|\n",
      "| Jefferson|     1|\n",
      "| Jefferson|     2|\n",
      "+----------+------+\n",
      "\n",
      "+----------+----+-----+\n",
      "|      name| key|value|\n",
      "+----------+----+-----+\n",
      "|     James| eye|brown|\n",
      "|     James|hair|black|\n",
      "|   Michael| eye| null|\n",
      "|   Michael|hair|brown|\n",
      "|    Robert| eye|     |\n",
      "|    Robert|hair|  red|\n",
      "|Washington|null| null|\n",
      "| Jefferson|null| null|\n",
      "+----------+----+-----+\n",
      "\n",
      "+---------+---+------+\n",
      "|     name|pos|   col|\n",
      "+---------+---+------+\n",
      "|    James|  0|  Java|\n",
      "|    James|  1| Scala|\n",
      "|  Michael|  0| Spark|\n",
      "|  Michael|  1|  Java|\n",
      "|  Michael|  2|  null|\n",
      "|   Robert|  0|CSharp|\n",
      "|   Robert|  1|      |\n",
      "|Jefferson|  0|     1|\n",
      "|Jefferson|  1|     2|\n",
      "+---------+---+------+\n",
      "\n",
      "+-------+---+----+-----+\n",
      "|   name|pos| key|value|\n",
      "+-------+---+----+-----+\n",
      "|  James|  0| eye|brown|\n",
      "|  James|  1|hair|black|\n",
      "|Michael|  0| eye| null|\n",
      "|Michael|  1|hair|brown|\n",
      "| Robert|  0| eye|     |\n",
      "| Robert|  1|hair|  red|\n",
      "+-------+---+----+-----+\n",
      "\n",
      "+----------+----+----+-----+\n",
      "|      name| pos| key|value|\n",
      "+----------+----+----+-----+\n",
      "|     James|   0| eye|brown|\n",
      "|     James|   1|hair|black|\n",
      "|   Michael|   0| eye| null|\n",
      "|   Michael|   1|hair|brown|\n",
      "|    Robert|   0| eye|     |\n",
      "|    Robert|   1|hair|  red|\n",
      "|Washington|null|null| null|\n",
      "| Jefferson|null|null| null|\n",
      "+----------+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# F.explode(col)\n",
    "# Returns a new row for each element in the given array or map.\n",
    "# When an array is passed to this function, it creates a new default column “col1” and it contains all array elements. \n",
    "# When a map is passed, it creates two new columns one for key and one for value and each element in map split into the rows.\n",
    "# F.explode will ignore elements that have null or empty, if there are nulls use explode_outer()\n",
    "\n",
    "# Show the initial dataframe\n",
    "df_explode.printSchema()\n",
    "df_explode.show()\n",
    "\n",
    "\n",
    "# F.explode() – array column example, this ignores nulls so be carefull when you use it\n",
    "df2 = df_explode.select(df_explode.name, explode(df_explode.knownLanguages))\n",
    "df2.printSchema()\n",
    "df2.show()\n",
    "\n",
    "\n",
    "# F.explode() – map column example, this ignores nulls so be carefull when you use it\n",
    "df3 = df_explode.select(df_explode.name, explode(df_explode.properties))\n",
    "df3.printSchema()\n",
    "df3.show()\n",
    "\n",
    "\n",
    "# F.explode_outer()\n",
    "# Create rows for each element in an array or map. For na values it creates null\n",
    "print(\"F.explode_outer() with array\")\n",
    "df_explode.select(df_explode.name, F.explode_outer(df_explode.knownLanguages)).show()\n",
    "\"\"\" with map \"\"\"\n",
    "df_explode.select(df_explode.name, F.explode_outer(df_explode.properties)).show()\n",
    "\n",
    "\n",
    "# F.posexplode()\n",
    "# creates a row for each element in the array and creates two columns “pos’ to hold the position of the array element and the ‘col’ to hold the actual array value.\n",
    "# This will ignore elements that have null or empty.\n",
    "\"\"\" with array \"\"\"\n",
    "df_explode.select(df_explode.name, F.posexplode(df_explode.knownLanguages)).show()\n",
    "\"\"\" with map \"\"\"\n",
    "df_explode.select(df_explode.name, F.posexplode(df_explode.properties)).show()\n",
    "\n",
    "\n",
    "# F.posexplode_outer()\n",
    "# creates a row for each element in the array and creates two columns “pos’ to hold the position of the array element and the ‘col’ to hold the actual array value.\n",
    "# Unlike posexplode, if the array or map is null or empty, posexplode_outer function returns null, null for pos and col columns.\n",
    "\"\"\" with array \"\"\"\n",
    "#df_explode.select($\"name\", F.posexplode_outer($\"knownLanguages\")).show()\n",
    "\"\"\" with map \"\"\"\n",
    "df_explode.select(df_explode.name, F.posexplode_outer(df_explode.properties)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|   name|gender|salary|\n",
      "+-------+------+------+\n",
      "|  James|     M| 60000|\n",
      "|Michael|     M| 70000|\n",
      "| Robert|  null|400000|\n",
      "|  Maria|     F|500000|\n",
      "|    Jen|      |  null|\n",
      "+-------+------+------+\n",
      "\n",
      "+-------+------+------+----------+\n",
      "|   name|gender|salary|new_gender|\n",
      "+-------+------+------+----------+\n",
      "|  James|     M| 60000|      Male|\n",
      "|Michael|     M| 70000|      Male|\n",
      "| Robert|  null|400000|          |\n",
      "|  Maria|     F|500000|    Female|\n",
      "|    Jen|      |  null|          |\n",
      "+-------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# F.when()\n",
    "# Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
    "# when(condition, value)\n",
    "df_when.show()\n",
    "df2 = df_when.withColumn(\"new_gender\", when(df_when.gender == \"M\",\"Male\")\n",
    "                                 .when(df_when.gender == \"F\",\"Female\")\n",
    "                                 .when(df_when.gender.isNull() ,\"\")\n",
    "                                 .otherwise(df_when.gender))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----+\n",
      "| id|           address|state|\n",
      "+---+------------------+-----+\n",
      "|  1|  14851 Jeffrey Rd|   DE|\n",
      "|  2|43421 Margarita St|   NY|\n",
      "|  3|  13111 Siemon Ave|   CA|\n",
      "+---+------------------+-----+\n",
      "\n",
      "+---+------------------+-----+\n",
      "| id|           address|state|\n",
      "+---+------------------+-----+\n",
      "|  1|14851 Jeffrey Road|   DE|\n",
      "|  2|43421 Margarita St|   NY|\n",
      "|  3|  13111 Siemon Ave|   CA|\n",
      "+---+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# F.regexp_replace()\n",
    "# WORKS ON: StringType\n",
    "# regexp_replace(str, pattern, replacement)\n",
    "# Replace all substrings of the specified string value that match regexp with rep.\n",
    "\n",
    "\n",
    "address = [(1,\"14851 Jeffrey Rd\",\"DE\"),\n",
    "    (2,\"43421 Margarita St\",\"NY\"),\n",
    "    (3,\"13111 Siemon Ave\",\"CA\")]\n",
    "df = spark.createDataFrame(address,[\"id\",\"address\",\"state\"])\n",
    "df.show()\n",
    "\n",
    "df.withColumn('address', F.regexp_replace('address', 'Rd', 'Road')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      "\n",
      "+---------+----------+--------+----------+\n",
      "|firstname|middlename|lastname|       dob|\n",
      "+---------+----------+--------+----------+\n",
      "|    James|          |   Smith|1991-04-01|\n",
      "|  Michael|      Rose|        |2000-05-19|\n",
      "|   Robert|          |Williams|1978-09-05|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|\n",
      "+---------+----------+--------+----------+\n",
      "\n",
      "+---------+----------+--------+----------+----+-----+---+\n",
      "|firstname|middlename|lastname|dob       |year|month|day|\n",
      "+---------+----------+--------+----------+----+-----+---+\n",
      "|James    |          |Smith   |1991-04-01|1991|04   |01 |\n",
      "|Michael  |Rose      |        |2000-05-19|2000|05   |19 |\n",
      "|Robert   |          |Williams|1978-09-05|1978|09   |05 |\n",
      "|Maria    |Anne      |Jones   |1967-12-01|1967|12   |01 |\n",
      "|Jen      |Mary      |Brown   |1980-02-17|1980|02   |17 |\n",
      "+---------+----------+--------+----------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# F.split()\n",
    "# split(str, pattern[, limit])\n",
    "# Splits a String (!!!) around matches of the given pattern.\n",
    "# If you want to split a date (01-01-1001), then you need to transform it to a string first\n",
    "# Alternatives exist how to split, make sure to check them out\n",
    "\n",
    "df_split.printSchema()\n",
    "df_split.show()\n",
    "df1 = df_split.withColumn('year', split(df_split['dob'], '-').getItem(0)) \\\n",
    "       .withColumn('month', split(df_split['dob'], '-').getItem(1)) \\\n",
    "       .withColumn('day', split(df_split['dob'], '-').getItem(2))\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n",
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|rank|\n",
      "+-------------+----------+------+----+\n",
      "|        James|     Sales|  3000|   1|\n",
      "|        James|     Sales|  3000|   1|\n",
      "|       Robert|     Sales|  4100|   3|\n",
      "|         Saif|     Sales|  4100|   3|\n",
      "|      Michael|     Sales|  4600|   5|\n",
      "|        Maria|   Finance|  3000|   1|\n",
      "|        Scott|   Finance|  3300|   2|\n",
      "|          Jen|   Finance|  3900|   3|\n",
      "|        Kumar| Marketing|  2000|   1|\n",
      "|         Jeff| Marketing|  3000|   2|\n",
      "+-------------+----------+------+----+\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# F.rank()\n",
    "# Window function: returns the rank of rows within a window partition.\n",
    "df_rank.show(truncate=False)\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "df_rank.withColumn(\"rank\", F.rank().over(windowSpec)).show()\n",
    "df_rank.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('James', 'Smith', 'USA', 'California'), ('Michael', 'Rose', 'USA', 'New York'), ('Robert', 'Williams', 'USA', 'California'), ('Maria', 'Jones', 'USA', 'Florida')]\n",
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n",
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|James    |Smith   |USA    |CA   |\n",
      "|Michael  |Rose    |USA    |NY   |\n",
      "|Robert   |Williams|USA    |CA   |\n",
      "|Maria    |Jones   |USA    |FL   |\n",
      "+---------+--------+-------+-----+\n",
      "\n",
      "+---------+--------+-------+----------+\n",
      "|firstname|lastname|country|state     |\n",
      "+---------+--------+-------+----------+\n",
      "|James    |Smith   |USA    |California|\n",
      "|Michael  |Rose    |USA    |New York  |\n",
      "|Robert   |Williams|USA    |California|\n",
      "|Maria    |Jones   |USA    |Florida   |\n",
      "+---------+--------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# F.broadcast(df)\n",
    "\"\"\"\n",
    "- https://sparkbyexamples.com/pyspark/pyspark-broadcast-variables/\n",
    "- Marks a DataFrame as small enough for use in broadcast joins.\n",
    "- In PySpark RDD and DataFrame, Broadcast variables are read-only shared variables that are cached and available on all nodes in a cluster in-order to access or use by the tasks. Instead of sending this data along with every task, PySpark distributes broadcast variables to the workers using efficient broadcast algorithms to reduce communication costs.\n",
    "- Broadcast variables are used in the same way for RDD, DataFrame.\n",
    "- When you run a PySpark RDD, DataFrame applications that have the Broadcast variables defined and used, PySpark does the following.\n",
    "    - PySpark breaks the job into stages that have distributed shuffling and actions are executed with in the stage.\n",
    "    - Later Stages are also broken into tasks\n",
    "    - Spark broadcasts the common data (reusable) needed by tasks within each stage.\n",
    "    - The broadcasted data is cache in serialized format and deserialized before executing each task.\n",
    "- When to use: You should be creating and using broadcast variables for data that shared across multiple stages and tasks. \n",
    "\"\"\"\n",
    "\n",
    "### How to use\n",
    "# The PySpark Broadcast is created using the broadcast(v) method of the SparkContext class.\n",
    "\n",
    "# PySpark RDD Broadcast variable example\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).collect()\n",
    "print(result)\n",
    "\n",
    "\n",
    "\n",
    "# PySpark DataFrame Broadcast variable example\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = df.rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+---------------+------------+-------------+--------+\n",
      "|            name| languagesAtSchool|languagesAtWork|currentState|previousState|     NEW|\n",
      "+----------------+------------------+---------------+------------+-------------+--------+\n",
      "|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|          OH|           CA|[OH, CA]|\n",
      "|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|          NY|           NJ|[NY, NJ]|\n",
      "|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|          UT|           NV|[UT, NV]|\n",
      "+----------------+------------------+---------------+------------+-------------+--------+\n",
      "\n",
      "+----------------+--------+\n",
      "|            name|  States|\n",
      "+----------------+--------+\n",
      "|    James,,Smith|[OH, CA]|\n",
      "|   Michael,Rose,|[NY, NJ]|\n",
      "|Robert,,Williams|[UT, NV]|\n",
      "+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# F.array()\n",
    "# Creates a new array column\n",
    "# Use array() function to create a new array column by merging the data from multiple columns. All input columns must have the same data type.\n",
    "# df = df.withColumn(\"Parch_New\", F.when(F.col(\"Parch\")==0, F.array(F.lit(\"666\"), F.lit(\"777\"))))\n",
    "# The value in the column goes from \"666\" -> \"[666]\" OR \"[666,777]\"\n",
    "df_array.show()\n",
    "\n",
    "df_array.select(df_array.name, F.array(df_array.currentState, df_array.previousState).alias(\"States\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `? -> Not sure about the code` not found.\n"
     ]
    }
   ],
   "source": [
    "# DF.repartition()\n",
    "# DataFrame.repartition(numPartitions, *cols)\n",
    "# Returns a new DataFrame partitioned by the given partitioning expressions. The resulting DataFrame is hash partitioned.\n",
    "# repartition() and coalesce() are very expensive operations as they shuffle the data across many partitions \n",
    "# repartition() is used to increase or decrease the RDD/DataFrame partitions\n",
    "??? -> Not sure about the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.withColumnRenamed()\n",
    "# Returns a new DataFrame by renaming an existing column\n",
    "# DataFrame.withColumnRenamed(existing, new)\n",
    "df1 = df.withColumnRenamed(\"dob\",\"DateOfBirth\").printSchema()\n",
    "df2 = df.withColumnRenamed(\"dob\",\"DateOfBirth\").withColumnRenamed(\"salary\",\"salary_amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|    James|   Smith|    USA|   CA|\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "|   Robert|Williams|    USA|   CA|\n",
      "|    Maria|   Jones|    USA|   FL|\n",
      "+---------+--------+-------+-----+\n",
      "\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|300000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|400000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|400000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|400000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|  -100|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n",
      "+---------+----------+--------+----------+------+------+------------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|CopiedColumn|\n",
      "+---------+----------+--------+----------+------+------+------------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|       -3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|       -4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|       -4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|       -4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|           1|\n",
      "+---------+----------+--------+----------+------+------+------------+\n",
      "\n",
      "+---------+----------+--------+----------+------+------+-------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|Country|\n",
      "+---------+----------+--------+----------+------+------+-------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|    USA|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|    USA|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|    USA|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|    USA|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|    USA|\n",
      "+---------+----------+--------+----------+------+------+-------+\n",
      "\n",
      "+---------+----------+--------+----------+------+------+-------+-------------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|Country|anotherColumn|\n",
      "+---------+----------+--------+----------+------+------+-------+-------------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|    USA| anotherValue|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|    USA| anotherValue|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|    USA| anotherValue|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|    USA| anotherValue|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|    USA| anotherValue|\n",
      "+---------+----------+--------+----------+------+------+-------+-------------+\n",
      "\n",
      "+---------+----------+--------+----------+---+------+\n",
      "|firstname|middlename|lastname|dob       |sex|salary|\n",
      "+---------+----------+--------+----------+---+------+\n",
      "|James    |          |Smith   |1991-04-01|M  |3000  |\n",
      "|Michael  |Rose      |        |2000-05-19|M  |4000  |\n",
      "|Robert   |          |Williams|1978-09-05|M  |4000  |\n",
      "|Maria    |Anne      |Jones   |1967-12-01|F  |4000  |\n",
      "|Jen      |Mary      |Brown   |1980-02-17|F  |-1    |\n",
      "+---------+----------+--------+----------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.withColumn()\n",
    "# DataFrame.withColumn(colName, col)\n",
    "# A transformation function of DF which used to change the value, convert the datatype of an existing column, create a new column, and many more.\n",
    "\n",
    "df.show()\n",
    "\n",
    "# 1. Change DataType using PySpark withColumn()\n",
    "df_wc.withColumn(\"salary\", col(\"salary\").cast(\"Integer\")).show()\n",
    "\n",
    "# 2. Update The Value of an Existing Column\n",
    "df_wc.withColumn(\"salary\", col(\"salary\")*100).show()\n",
    "\n",
    "# 3. Create a Column from an Existing\n",
    "df_wc.withColumn(\"CopiedColumn\", col(\"salary\")* -1).show()\n",
    "\n",
    "# 4. Add a New Column using withColumn()\n",
    "df_wc.withColumn(\"Country\", lit(\"USA\")).show()\n",
    "df_wc.withColumn(\"Country\", lit(\"USA\")) \\\n",
    "  .withColumn(\"anotherColumn\",lit(\"anotherValue\")) \\\n",
    "  .show()\n",
    "\n",
    "# 5. Rename Column Name\n",
    "df_wc.withColumnRenamed(\"gender\",\"sex\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|2021-01-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2021-01-01|     M|  4000|\n",
      "|   Robert|          |Williams|2021-01-01|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|2021-01-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|2021-01-01|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n",
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: date (nullable = false)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime.date(2021, 1, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataFrame.schema()\n",
    "# Returns the schema of this DataFrame as a pyspark.sql.types.StructType\n",
    "df_wc_two.show()\n",
    "df_wc_two.printSchema()\n",
    "maxdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|  Michael|      Rose|        |2021-01-01|     M|  4000|\n",
      "|   Robert|          |Williams|2021-01-01|     M|  4000|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame.agg(*exprs)\n",
    "# Aggregate on the entire DataFrame without groups (shorthand for df.groupBy().agg())\n",
    "\n",
    "# Example\n",
    "# Get the most recent date/file/record a dataset contains\n",
    "df_wc_two = df_wc.withColumn(\"dob\", df_wc.dob.cast(DateType()))\n",
    "maxdate = df_wc_two.agg({\"dob\": \"max\"}).collect()[0][0]\n",
    "df_wc_two.filter(F.col(\"dob\")==maxdate).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n",
      "[Row(dept_name='Finance', dept_id=10), Row(dept_name='Marketing', dept_id=20), Row(dept_name='Sales', dept_id=30), Row(dept_name='IT', dept_id=40)]\n"
     ]
    }
   ],
   "source": [
    "# DataFrame.collect()\n",
    "# Returns all the records as a list of Row.\n",
    "# PySpark RDD/DataFrame collect() is an action operation that is used to retrieve all the elements of the dataset (from all nodes) to the driver node. We should use the collect() on smaller dataset usually after filter(), group() e.t.c. Retrieving larger datasets results in OutOfMemory error\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.show(truncate=False)\n",
    "\n",
    "# deptDF.collect() retrieves all elements in a DataFrame as an Array of Row type to the driver node. printing a resultant array yields the below output.\n",
    "# Note that collect() is an action hence it does not return a DataFrame instead, it returns data in an Array to the driver.\n",
    "dataCollect = deptDF.collect()\n",
    "print(dataCollect)\n",
    "\n",
    "\n",
    "# When to avoid collect\n",
    "# Usually, collect() is used to retrieve the action output when you have very small result set and calling collect() on an RDD/DataFrame with a bigger result set causes out of memory as it returns the entire dataset (from all workers) to the driver hence we should avoid calling collect() on a larger dataset.\n",
    "\n",
    "# collect () vs select ()\n",
    "# select() is a transformation that returns a new DataFrame and holds the columns that are selected whereas collect() is an action that returns the entire data set in an Array to the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n",
      "+------------------------------------------------------------+\n",
      "|collect_list(salary)                                        |\n",
      "+------------------------------------------------------------+\n",
      "|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n",
      "+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# F.collect_list()\n",
    "# Returns all values from an input column with duplicates.\n",
    "df_collectlist.show(truncate=False)\n",
    "df_collectlist.select(F.collect_list(\"salary\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------+\n",
      "|collect_set(employee_name)                                    |\n",
      "+--------------------------------------------------------------+\n",
      "|[Robert, Kumar, Jeff, Maria, Scott, Michael, Saif, Jen, James]|\n",
      "+--------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# collect_set\n",
    "# Returns all values from an input column with duplicate values eliminated. \n",
    "\n",
    "df_collectlist.select(F.collect_set(\"employee_name\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------------+-----+------+-------+\n",
      "|name                  |languages         |state|gender|gender2|\n",
      "+----------------------+------------------+-----+------+-------+\n",
      "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |Male   |\n",
      "|{Anna, Rose, }        |[Spark, Java, C++]|NY   |F     |Female |\n",
      "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |Male   |\n",
      "|{Maria, Anne, Jones}  |[CSharp, VB]      |NY   |M     |Female |\n",
      "|{Jen, Mary, Brown}    |[CSharp, VB]      |NY   |M     |Male   |\n",
      "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |Female |\n",
      "+----------------------+------------------+-----+------+-------+\n",
      "\n",
      "+----------------------+------------------+-----+------+-------+\n",
      "|name                  |languages         |state|gender|gender2|\n",
      "+----------------------+------------------+-----+------+-------+\n",
      "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |Male   |\n",
      "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |Male   |\n",
      "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |Female |\n",
      "+----------------------+------------------+-----+------+-------+\n",
      "\n",
      "+----------------------+------------------+-----+------+-------+\n",
      "|name                  |languages         |state|gender|gender2|\n",
      "+----------------------+------------------+-----+------+-------+\n",
      "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |Male   |\n",
      "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |Male   |\n",
      "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |Female |\n",
      "+----------------------+------------------+-----+------+-------+\n",
      "\n",
      "+----------------------+------------------+-----+------+-------+\n",
      "|name                  |languages         |state|gender|gender2|\n",
      "+----------------------+------------------+-----+------+-------+\n",
      "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |Male   |\n",
      "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |Female |\n",
      "+----------------------+------------------+-----+------+-------+\n",
      "\n",
      "+--------------------+------------------+-----+------+-------+\n",
      "|                name|         languages|state|gender|gender2|\n",
      "+--------------------+------------------+-----+------+-------+\n",
      "|    {James, , Smith}|[Java, Scala, C++]|   OH|     M|   Male|\n",
      "| {Julia, , Williams}|      [CSharp, VB]|   OH|     F|   Male|\n",
      "|{Mike, Mary, Will...|      [Python, VB]|   OH|     M| Female|\n",
      "+--------------------+------------------+-----+------+-------+\n",
      "\n",
      "+--------------------+------------------+-----+------+-------+\n",
      "|                name|         languages|state|gender|gender2|\n",
      "+--------------------+------------------+-----+------+-------+\n",
      "|      {Anna, Rose, }|[Spark, Java, C++]|   NY|     F| Female|\n",
      "|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M| Female|\n",
      "|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|   Male|\n",
      "+--------------------+------------------+-----+------+-------+\n",
      "\n",
      "+--------------------+------------------+-----+------+-------+\n",
      "|                name|         languages|state|gender|gender2|\n",
      "+--------------------+------------------+-----+------+-------+\n",
      "|      {Anna, Rose, }|[Spark, Java, C++]|   NY|     F| Female|\n",
      "|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M| Female|\n",
      "|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|   Male|\n",
      "+--------------------+------------------+-----+------+-------+\n",
      "\n",
      "+--------------------+------------------+-----+------+-------+\n",
      "|                name|         languages|state|gender|gender2|\n",
      "+--------------------+------------------+-----+------+-------+\n",
      "|      {Anna, Rose, }|[Spark, Java, C++]|   NY|     F| Female|\n",
      "|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M| Female|\n",
      "|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|   Male|\n",
      "+--------------------+------------------+-----+------+-------+\n",
      "\n",
      "+--------------------+------------------+-----+------+-------+\n",
      "|                name|         languages|state|gender|gender2|\n",
      "+--------------------+------------------+-----+------+-------+\n",
      "|    {James, , Smith}|[Java, Scala, C++]|   OH|     M|   Male|\n",
      "| {Julia, , Williams}|      [CSharp, VB]|   OH|     F|   Male|\n",
      "|{Mike, Mary, Will...|      [Python, VB]|   OH|     M| Female|\n",
      "+--------------------+------------------+-----+------+-------+\n",
      "\n",
      "+--------------------+------------------+-----+------+-------+\n",
      "|                name|         languages|state|gender|gender2|\n",
      "+--------------------+------------------+-----+------+-------+\n",
      "|    {James, , Smith}|[Java, Scala, C++]|   OH|     M|   Male|\n",
      "| {Julia, , Williams}|      [CSharp, VB]|   OH|     F|   Male|\n",
      "|{Mike, Mary, Will...|      [Python, VB]|   OH|     M| Female|\n",
      "+--------------------+------------------+-----+------+-------+\n",
      "\n",
      "+--------------------+------------+-----+------+-------+\n",
      "|                name|   languages|state|gender|gender2|\n",
      "+--------------------+------------+-----+------+-------+\n",
      "| {Julia, , Williams}|[CSharp, VB]|   OH|     F|   Male|\n",
      "|{Maria, Anne, Jones}|[CSharp, VB]|   NY|     M| Female|\n",
      "+--------------------+------------+-----+------+-------+\n",
      "\n",
      "+----------------+------------------+-----+------+-------+\n",
      "|name            |languages         |state|gender|gender2|\n",
      "+----------------+------------------+-----+------+-------+\n",
      "|{James, , Smith}|[Java, Scala, C++]|OH   |M     |Male   |\n",
      "|{Anna, Rose, }  |[Spark, Java, C++]|NY   |F     |Female |\n",
      "+----------------+------------------+-----+------+-------+\n",
      "\n",
      "+----------------------+------------+-----+------+-------+\n",
      "|name                  |languages   |state|gender|gender2|\n",
      "+----------------------+------------+-----+------+-------+\n",
      "|{Julia, , Williams}   |[CSharp, VB]|OH   |F     |Male   |\n",
      "|{Mike, Mary, Williams}|[Python, VB]|OH   |M     |Female |\n",
      "+----------------------+------------+-----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame.filter(condition)\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-where-filter/\n",
    "# Filters rows using the given condition\n",
    "# Use this to subset a dataset\n",
    "df_filter.show(truncate=False)\n",
    "\n",
    "\n",
    "# Both of these produce the same results\n",
    "df_filter.filter(df_filter.state == \"OH\").show(truncate=False)\n",
    "df_filter.filter(F.col(\"state\") == \"OH\").show(truncate=False) \n",
    "\n",
    "# Multiple Conditions\n",
    "df_filter.filter( (df_filter.state  == \"OH\") & (df_filter.gender  == \"M\") ).show(truncate=False) \n",
    "\n",
    "#Filter IS IN List values\n",
    "li=[\"OH\",\"CA\",\"DE\"]\n",
    "df_filter.filter(df_filter.state.isin(li)).show()\n",
    "\n",
    "# Filter NOT IS IN List values\n",
    "#These show all records with NY (NY is not part of the list)\n",
    "df_filter.filter(~df_filter.state.isin(li)).show()\n",
    "df_filter.filter(df_filter.state.isin(li)==False).show()\n",
    "\n",
    "# Using startswith\n",
    "df_filter.filter(df_filter.state.startswith(\"N\")).show()\n",
    "\n",
    "#using endswith\n",
    "df_filter.filter(df_filter.state.endswith(\"H\")).show()\n",
    "\n",
    "#contains\n",
    "df_filter.filter(df_filter.state.contains(\"H\")).show()\n",
    "\n",
    "# like - SQL LIKE pattern\n",
    "#df_filter.filter(df_filter.gender2.like(\"%ale%\")).show()\n",
    "df_filter.filter(df_filter.name.firstname.like(\"%ia%\")).show()\n",
    "\n",
    "# Filter on Array collumn\n",
    "from pyspark.sql.functions import array_contains\n",
    "df_filter.filter(array_contains(df_filter.languages,\"Java\")).show(truncate=False) \n",
    "\n",
    "# Filter on nested Struct condition\n",
    "df_filter.filter(df_filter.name.lastname == \"Williams\").show(truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame.toDF(*cols)\n",
    "# Returns a new DataFrame that with new specified column names\n",
    "# https://sparkbyexamples.com/pyspark/different-ways-to-create-dataframe-in-pyspark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|James    |Smith   |USA    |CA   |\n",
      "|Michael  |Rose    |USA    |NY   |\n",
      "|Robert   |Williams|USA    |CA   |\n",
      "|Maria    |Jones   |USA    |FL   |\n",
      "+---------+--------+-------+-----+\n",
      "\n",
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "|    Maria|   Jones|\n",
      "+---------+--------+\n",
      "\n",
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "|    Maria|   Jones|\n",
      "+---------+--------+\n",
      "\n",
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "|    Maria|   Jones|\n",
      "+---------+--------+\n",
      "\n",
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "|    Maria|   Jones|\n",
      "+---------+--------+\n",
      "\n",
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "|    Maria|   Jones|\n",
      "+---------+--------+\n",
      "\n",
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|    James|   Smith|    USA|   CA|\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "|   Robert|Williams|    USA|   CA|\n",
      "|    Maria|   Jones|    USA|   FL|\n",
      "+---------+--------+-------+-----+\n",
      "\n",
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|    James|   Smith|    USA|   CA|\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "|   Robert|Williams|    USA|   CA|\n",
      "|    Maria|   Jones|    USA|   FL|\n",
      "+---------+--------+-------+-----+\n",
      "\n",
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|    James|   Smith|    USA|   CA|\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "|   Robert|Williams|    USA|   CA|\n",
      "|    Maria|   Jones|    USA|   FL|\n",
      "+---------+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame.select(*cols)\n",
    "# Projects a set of expressions and returns a new DataFrame\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")]\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Selecting single and multiple columns\n",
    "df.select(\"firstname\",\"lastname\").show()\n",
    "df.select(df.firstname,df.lastname).show()\n",
    "df.select(df[\"firstname\"],df[\"lastname\"]).show()\n",
    "#By using col() function\n",
    "df.select(col(\"firstname\"),col(\"lastname\")).show()\n",
    "#Select columns by regular expression\n",
    "df.select(df.colRegex(\"`^.*name*`\")).show()\n",
    "\n",
    "\n",
    "# Select all columns\n",
    "# Select All columns from List\n",
    "df.select(*columns).show()\n",
    "# Select All columns\n",
    "df.select([col for col in df.columns]).show()\n",
    "df.select(\"*\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|    James|   Smith|    USA|   CA|\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "|   Robert|Williams|    USA|   CA|\n",
      "|    Maria|   Jones|    USA|   FL|\n",
      "+---------+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DF.drop()\n",
    "# Drop Column From PySpark DataFrame\n",
    "df.drop(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterfunction():\n",
    "    for i in range(20):\n",
    "        print(\"Bobi e naipametan i naiubav i naidobar ujko!!!\")\n",
    "\n",
    "filterfunction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataFrame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-d3bf984a2c01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Return a new DataFrame containing union of rows in this and another DataFrame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'DataFrame' is not defined"
     ]
    }
   ],
   "source": [
    "# DF.union()\n",
    "# Return a new DataFrame containing union of rows in this and another DataFrame.\n",
    "df1.union(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF.join(DF)\n",
    "\n",
    "# One condition\n",
    "df1.join(df2, )\n",
    "# Two conditions\n",
    "df1.join(df2, ((df1.col1==df2.col1) & (df1.col8==df2.col8)), \"left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Column API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F.col()\n",
    "# Returns a Column based on the given column name.\n",
    "df.select(F.col(\"name\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column.asc()\n",
    "# Returns a sort expression based on ascending order of the column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|   name|gender|salary|\n",
      "+-------+------+------+\n",
      "|  James|     M|   100|\n",
      "|Michael|     M|   400|\n",
      "| Robert|  null|   500|\n",
      "|  Maria|     F|   800|\n",
      "|    Jen|      |  null|\n",
      "+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Column.when(condition, value)\n",
    "# Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
    "data = [(\"James\",\"M\",100),\n",
    "        (\"Michael\",\"M\",400),\n",
    "        (\"Robert\",None,500),\n",
    "        (\"Maria\",\"F\",800),\n",
    "        (\"Jen\",\"\",None)]\n",
    "\n",
    "columns = [\"name\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show()\n",
    "\n",
    "df2 = df.withColumn(\"new_gender\", when(df.gender == \"M\",\"Male\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+\n",
      "|   name|gender|salary|new_gender|\n",
      "+-------+------+------+----------+\n",
      "|  James|     M|   100|      Male|\n",
      "|Michael|     M|   400|      Male|\n",
      "| Robert|  null|   500|      null|\n",
      "|  Maria|     F|   800|         F|\n",
      "|    Jen|      |  null|          |\n",
      "+-------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Column.otherwise(value)\n",
    "# Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
    "df = df.withColumn(\"new_gender\", when(df.gender == \"M\",\"Male\").otherwise(df.gender))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|(salary IS NOT NULL)|\n",
      "+--------------------+\n",
      "|                true|\n",
      "|                true|\n",
      "|                true|\n",
      "|                true|\n",
      "|               false|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Column.isNotNull()\n",
    "# True if the current expression is NOT null.\n",
    "df.select(col(\"salary\").isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|(salary IS NULL)|\n",
      "+----------------+\n",
      "|           false|\n",
      "|           false|\n",
      "|           false|\n",
      "|           false|\n",
      "|            true|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Column.isNull()\n",
    "# True if the current expression is null.\n",
    "df.select(col(\"salary\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+\n",
      "|((salary >= 200) AND (salary <= 700))|\n",
      "+-------------------------------------+\n",
      "|                                false|\n",
      "|                                false|\n",
      "|                                false|\n",
      "+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Column.between(lowerBound, upperBound)\n",
    "df.select(col(\"salary\").between(200,700)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: int]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column.cast(dataType)\n",
    "# Convert the column into type dataType\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "simpleData = [(\"James\",34,\"2006-01-01\",\"true\",\"M\",3000.60),\n",
    "    (\"Michael\",33,\"1980-01-10\",\"true\",\"F\",3300.80),\n",
    "    (\"Robert\",37,\"06-01-1992\",\"false\",\"M\",5000.50)\n",
    "  ]\n",
    "\n",
    "columns = [\"firstname\",\"age\",\"jobStartDate\",\"isGraduated\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "\n",
    "# Convert String to Integer Type\n",
    "df.withColumn(\"age\", df.age.cast(IntegerType()))\n",
    "df.withColumn(\"age\", df.age.cast('int'))\n",
    "df.withColumn(\"age\", df.age.cast('integer'))\n",
    "\n",
    "# Using select\n",
    "df.select(col(\"age\").cast('int').alias(\"age\"))\n",
    "\n",
    "#Using selectExpr()\n",
    "df.selectExpr(\"cast(age as int) age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dates and Time Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|current_date|\n",
      "+------------+\n",
      "|  2021-10-12|\n",
      "|  2021-10-12|\n",
      "|  2021-10-12|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.current_date().alias(\"current_date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Else\n",
    "- F.udf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Do\n",
    "\n",
    "-\n",
    "\n",
    "- F.month()\n",
    "- F.to_date()\n",
    "\n",
    "- .repartition()\n",
    "- .distinct()\n",
    "\n",
    "- .like()\n",
    "- .orderBy()\n",
    "- .over()\n",
    "- alerts.join()\n",
    "\n",
    "- utils.union_hubs()\n",
    "- workflow=workflow.select()\n",
    "- Window.partitionBy()\n",
    "\n",
    "-df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "from pyspark.sql.functions import array_distinct\n",
    "B = [\"Bobi\", \"Blerina\", \"Misina\", \"Tyler\"]\n",
    "A = [\"Bobi\", \"Blerina\", \"Misina\", \"Leo\"]\n",
    "\n",
    "def doubles(B,A):\n",
    "    unique = []\n",
    "    for i in A:\n",
    "        unique.append(i)\n",
    "    for i in B:\n",
    "        unique.append(i)\n",
    "    return unique\n",
    "\n",
    "dub = doubles(B, A)\n",
    "#array_distinct(dub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_column(df, prefix):\n",
    "    '''\n",
    "    This is the function we use at CS to rename columns\n",
    "    '''\n",
    "    for name in df.columns:\n",
    "        if prefix not in name:\n",
    "            df_new = df.withColumnRenamed(name, prefix + \"__\" + name)\n",
    "            df = df_new\n",
    "        else:\n",
    "            raise ValueError('The chosen prefix already appers in the dataframe.')\n",
    "    return df\n",
    "\n",
    "titanic_new = rename_column(titanic, 'OK')\n",
    "titanic_new.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bobis_function():\n",
    "    for i in range(1):\n",
    "        print(\"Bujaa\")\n",
    "        \n",
    "bobis_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "93d105ada02192d0bbdad0d4b7f9e51b851ebd5be727b9bab0a9679568b88653"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('pyspark_env': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "f8bd098fb45a3bbd2cd1cd7c5b8c0d25015d78db0dd1c22a72d4edf2a00daf2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
