{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit ('pyspark_env': conda)"
  },
  "interpreter": {
   "hash": "f8bd098fb45a3bbd2cd1cd7c5b8c0d25015d78db0dd1c22a72d4edf2a00daf2c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "###############################################################################\n",
    "# Loops\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "https://medium.com/@rtjeannier/pandas-101-cont-9d061cb73bfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace strings in Domainnames\n",
    "# Just an incomplete example so the regex does not clean the data correctly\n",
    "domain                              host                                 url\n",
    "erotikchatz.de                      erotikchatz.de                       erotikchatz.de\n",
    "erotiklife.de                       erotiklife.de                        erotiklife.de\n",
    "erstberlin.de                       erstberlin.de                        erstberlin.de\n",
    "\n",
    "\n",
    "nom = pd.read_json(r\"D:\\6_Rohdaten\\Crawling\\Nominet\\nominet_domains.json\", orient='record', encoding='utf_8', lines=False)\n",
    "nomloop = nom.iloc[1:200:,0:3]\n",
    "def myreplace(s):\n",
    "    for ch in [r'www.', r'http', '://', \"/\"]:\n",
    "        s = s.replace(ch, \"\")\n",
    "    return s\n",
    "nomloop[\"url\"] = nomloop[\"url\"].map(myreplace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# This is an example how you cleaned data for the Naive Bayes\n",
    "###############################################################################\n",
    "#Testdata\n",
    "review,label\n",
    "The picture is clear and beautiful,positive\n",
    "Picture is not clear,negative\n",
    "#Traindata\n",
    "review,label\n",
    "Colors & clarity is superb,positive\n",
    "Sadly the picture is not nearly as clear or bright as my 40 inch Samsung,negative\n",
    "# This in Files\n",
    "test = r\"D:\\Git\\fakeshop_analysis\\Scikit_NB\\Example\\test.txt\"\n",
    "train = r\"D:\\Git\\fakeshop_analysis\\Scikit_NB\\Example\\train.txt\"\n",
    "\n",
    "def load_data(filename):\n",
    "    \"\"\" This function works, but you have to modify the second-to-last line from\n",
    "    reviews.append(line[0].split()) to reviews.append(line[0]).\n",
    "    CountVectorizer will perform the splits by itself as it sees fit, trust him :)\"\"\"\n",
    "    reviews = list()\n",
    "    labels = list()\n",
    "    with open(filename) as file:\n",
    "        file.readline()\n",
    "        for line in file:\n",
    "            line = line.strip().split(',')\n",
    "            labels.append(line[1])\n",
    "            reviews.append(line[0])\n",
    "    return reviews, labels\n",
    "\n",
    "X_train, y_train = load_data(train)\n",
    "X_test, y_test   = load_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# REGEX vs ahocorastick Algorithm search: \n",
    "###############################################################################\n",
    "https://stackoverflow.com/questions/48541444/pandas-filtering-for-multiple-substrings-in-series/48600345#48600345\n",
    "https://pyahocorasick.readthedocs.io/en/latest/\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import ahocorasick\n",
    "import re\n",
    "\n",
    "random.seed(321)\n",
    "\n",
    "def orig(col, lst):\n",
    "    mask = np.logical_or.reduce([col.str.contains(i, regex=False, case=False) \n",
    "                                 for i in lst])\n",
    "    return mask\n",
    "\n",
    "def using_regex(col, lst):\n",
    "    \"\"\"https://stackoverflow.com/a/48590850/190597 (Alex Riley)\"\"\"\n",
    "    esc_lst = [re.escape(s) for s in lst]\n",
    "    pattern = '|'.join(esc_lst)\n",
    "    mask = col.str.contains(pattern, case=False)\n",
    "    return mask\n",
    "\n",
    "def using_ahocorasick(col, lst):\n",
    "    A = ahocorasick.Automaton(ahocorasick.STORE_INTS)\n",
    "    for word in lst:\n",
    "        A.add_word(word.lower())\n",
    "    A.make_automaton() \n",
    "    col = col.str.lower()\n",
    "    mask = col.apply(lambda x: bool(list(A.iter(x))))\n",
    "    return mask\n",
    "\n",
    "N = 50000\n",
    "# 100 substrings of 5 characters\n",
    "lst = [''.join([chr(random.randint(0, 256)) for _ in range(5)]) for _ in range(100)]\n",
    "\n",
    "# N strings of 20 characters\n",
    "strings = [''.join([chr(random.randint(0, 256)) for _ in range(20)]) for _ in range(N)]\n",
    "# make about 10% of the strings match a string from lst; this helps check that our method works\n",
    "strings = [_ if random.randint(0, 99) < 10 else _+random.choice(lst) for _ in strings]\n",
    "\n",
    "col = pd.Series(strings)\n",
    "\n",
    "expected = orig(col, lst)\n",
    "for name, result in [('using_regex', using_regex(col, lst)),\n",
    "                     ('using_ahocorasick', using_ahocorasick(col, lst))]:\n",
    "    status = 'pass' if np.allclose(expected, result) else 'fail'\n",
    "    print('{}: {}'.format(name, status))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Parse Datetes using this package in a loop\n",
    "###############################################################################\n",
    "from dateutil.parser import parse\n",
    "\n",
    "date_array = [\n",
    "    '2018-06-29 08:15:27.243860',\n",
    "    'Jun 28 2018  7:40AM',\n",
    "    'Jun 28 2018 at 7:40AM',\n",
    "    'September 18, 2017, 22:19:55',\n",
    "    'Sun, 05/12/1999, 12:30PM',\n",
    "    'Mon, 21 March, 2015',\n",
    "    '2018-03-12T10:12:45Z',\n",
    "    '2018-06-29 17:08:00.586525+00:00',\n",
    "    '2018-06-29 17:08:00.586525+05:00',\n",
    "    'Tuesday , 6th September, 2017 at 4:30pm'\n",
    "]\n",
    "\n",
    "for date in date_array:\n",
    "    print('Parsing: ' + date)\n",
    "    dt = parse(date)\n",
    "    print(dt.date())\n",
    "    print(dt.time())\n",
    "    print(dt.tzinfo)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Multiply a list of numbes with one singel number \n",
    "###############################################################################\n",
    "# Loopy loop\n",
    "list = [1,2,3,4,5,6,7,8,9]\n",
    "my_new_list = []\n",
    "for i in list:\n",
    "    my_new_list.append(i * (24*60*60))\n",
    "\n",
    "# Or the same with a list comprehension\n",
    "my_list = [1, 2, 3, 4, 5]\n",
    "my_new_list = [i * 5 for i in my_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Function to count NA's in a Dataframe\n",
    "###############################################################################\n",
    "#https://stackoverflow.com/questions/26266362/how-to-count-the-nan-values-in-a-column-in-pandas-dataframe\n",
    "def missing_values_table(df):\n",
    "        mis_val = df.isnull().sum()\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Split\n",
    "# https://www.datacamp.com/community/tutorials/categorical-data\n",
    "###############################################################################\n",
    "dummy_df_age = pd.DataFrame({'age': ['0-20', '20-40', '40-60','60-80']})\n",
    "dummy_df_age['start'], dummy_df_age['end'] = zip(*dummy_df_age['age'].map(lambda x: x.split('-')))\n",
    "\n",
    "dummy_df_age.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Replace a range with its mean\n",
    "# https://www.datacamp.com/community/tutorials/categorical-data\n",
    "###############################################################################\n",
    "dummy_df_age = pd.DataFrame({'age': ['0-20', '20-40', '40-60','60-80']})\n",
    "\n",
    "def split_mean(x):\n",
    "    split_list = x.split('-')\n",
    "    mean = (float(split_list[0])+float(split_list[1]))/2\n",
    "    return mean\n",
    "\n",
    "dummy_df_age['age_mean'] = dummy_df_age['age'].apply(lambda x: split_mean(x))\n",
    "\n",
    "dummy_df_age.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# \n",
    "###############################################################################\n",
    "dummy_df_age = pd.DataFrame({'age': ['0-20', '20-40', '40-60','60-80']})\n",
    "\n",
    "def split_mean(x):\n",
    "    split_list = x.split('-')\n",
    "    mean = (float(split_list[0])+float(split_list[1]))/2\n",
    "    return mean\n",
    "\n",
    "dummy_df_age['age_mean'] = dummy_df_age['age'].apply(lambda x: split_mean(x))\n",
    "\n",
    "dummy_df_age.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Geron: Chapter3\n",
    "###############################################################################\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Geron Decsision Trees \n",
    "###############################################################################\n",
    "a. Continuing the previous exercise, generate 1,000 subsets of the training set, each containing 100 instances selected randomly. Hint: you can use Scikit-Learn's ShuffleSplit class for this.\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "n_trees = 1000\n",
    "n_instances = 100\n",
    "\n",
    "mini_sets = []\n",
    "\n",
    "rs = ShuffleSplit(n_splits=n_trees, test_size=len(X_train) - n_instances, random_state=42)\n",
    "for mini_train_index, mini_test_index in rs.split(X_train):\n",
    "    X_mini_train = X_train[mini_train_index]\n",
    "    y_mini_train = y_train[mini_train_index]\n",
    "    mini_sets.append((X_mini_train, y_mini_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Identify langages\n",
    "###############################################################################\n",
    "\n",
    "from textblob import TextBlob\n",
    "def detect_language(text):\n",
    "    if len(text)>3:\n",
    "        r=TextBlob(text)\n",
    "        lang = r.detect_language()\n",
    "        return lang\n",
    "dfx['language']=dfx.html_body_3.apply(lambda x: detect_language(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Write a function that constructs this vocabulary\n",
    "###############################################################################\n",
    "As an example, say that we were to use one-hot encoding for the following sentences:\n",
    "\n",
    "s1. \"This is sentence one.\"\n",
    "s2. \"Now, here is our sentence number two.\"\n",
    "\n",
    "The vocabulary from the two sentences is:\n",
    "\n",
    "vocabulary = {\"here\": 0, \"is\": 1, \"now\": 2, \"number\": 3, \"one\": 4, \"our\": 5, \"sentence\": 6, \"this\": 7, \"two\": 8}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Write this nltk/mlp task\n",
    "###############################################################################\n",
    "\n",
    "Take a document as the input.\n",
    "Read the document line by line\n",
    "Tokenize the line\n",
    "Stem the words\n",
    "Output the stemmed words (print on screen or write to a file)\n",
    "Repeat step 2 to step 5 until it is to the end of the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Write this\n",
    "###############################################################################\n",
    "1. Take a couple of different documents like pdfs with different topics (Economics, Engineering, Flowers, ...)\n",
    "2 Extract the text\n",
    "3. Print the most frequent words per Topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Document Clustering\n",
    "###############################################################################\n",
    "Download the nltk.gutenberg books and perform a cluster analysis on them to see which books are similar\n",
    "Do this after having read the chapter about clustering\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Practice the python .apply function\n",
    "###############################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Figure out how to apply this to to a column\n",
    "###############################################################################\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('de_core_news_lg')\n",
    "\n",
    "mails=['Hallo. Ich spielte am frühen Morgen und ging dann zu einem Freund. Auf Wiedersehen', 'Guten Tag Ich mochte Bälle und will etwas kaufen. Tschüss']\n",
    "\n",
    "mails_lemma = []\n",
    "\n",
    "for mail in mails:\n",
    "     doc = nlp(mail)\n",
    "     result = ''\n",
    "     for token in doc:\n",
    "        result += token.lemma_\n",
    "        result += ' ' \n",
    "     mails_lemma.append(result)\n",
    "print(mails_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# A small practice problem\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "Take this Loop and modify it by applying a function on those words like len\n",
    "The result should be a DF with two columns:\n",
    "C1: the words in thislist\n",
    "C2: the length of each word\n",
    "\n",
    "Try it in two different ways:\n",
    "Way 2: Us e a DF and write the result to that DF\n",
    "\n",
    "You can also vary the task by preprocessing more like: strip whitespace-> lowercase->length\n",
    "\"\"\"\n",
    "\n",
    "# Way 1: Use a list and write the results to a list\n",
    "empty = []\n",
    "thislist = [\"apple\", \"banana\", \"cherry\", \"Bujaaa\", \"  Holy  Friholes! \"]\n",
    "for x in thislist:\n",
    "    temp1 = x.lower()\n",
    "    temp2 = temp1.replace(' ', '')\n",
    "    temp3 = len(temp2)\n",
    "    empty.append(temp3)\n",
    "    print(temp3) \n",
    "\n",
    "\n",
    "# Way 2: Us e a DF and write the result to that DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# A small practice problem\n",
    "###############################################################################\n",
    "# Split the items in two parts\n",
    "# Save them in separate lists\n",
    "# Count the number of characters in each\n",
    "\n",
    "thislist = [\"AA 123\", \"AB 1234\", \"AC 12345\", \"AD 123456\", \"AE 1234567\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "###############################################################################\n",
    "# A small practice problem\n",
    "###############################################################################\n",
    "# Take a df which contains text\n",
    "# write a loop which goes trough the text and looks for words\n",
    "# Every new word found is added to a list\n",
    "# The list should contain all unique words which can be found in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# A small practice problem\n",
    "###############################################################################\n",
    "\n",
    "###############################################################################\n",
    "# A small practice problem\n",
    "###############################################################################\n",
    "\n",
    "###############################################################################\n",
    "# A small practice problem\n",
    "###############################################################################\n",
    "\n",
    "###############################################################################\n",
    "# A small practice problem\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "T\nE\nS\nT\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# A small practice problem\n",
    "###############################################################################\n",
    "\n",
    "list1 = [\"geeks\", \"for\", \"geeks\"]\n",
    "list2 = (\"geeks\", \"for\", \"geeks\", \"squared\")\n",
    "string1 = \"test\"\n",
    "\n",
    "for i in string1:\n",
    "    print(i.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "current letter : g\ncurrent letter : k\ncurrent letter : f\ncurrent letter : o\ncurrent letter : r\ncurrent letter : g\ncurrent letter : k\n"
     ]
    }
   ],
   "source": [
    "for letter in \"geeksforgeeks\":\n",
    "    if letter == \"e\" or letter ==\"s\":\n",
    "        continue\n",
    "    print(\"current letter :\", letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "current letter:  e\ncurrent letter:  e\ncurrent letter:  k\ncurrent letter:  s\ncurrent letter:  f\ncurrent letter:  o\ncurrent letter:  r\ncurrent letter:  e\ncurrent letter:  e\ncurrent letter:  k\ncurrent letter:  s\n"
     ]
    }
   ],
   "source": [
    "geeks = \"geeksforgeeks\"\n",
    "for letter in geeks:\n",
    "    if letter==\"i\" or letter==\"g\":\n",
    "        continue\n",
    "    print(\"current letter: \", letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "Exception",
     "evalue": "No Way!",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-69fb65434bb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mletter\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"g\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"No Way!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Current Letter:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mletter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: No Way!"
     ]
    }
   ],
   "source": [
    "geeks = \"geeksforgeeks\"\n",
    "for letter in geeks:\n",
    "    if letter==\"z\" or letter==\"z\":\n",
    "        break\n",
    "    if letter==\"g\":\n",
    "        raise Exception (\"No Way!\")\n",
    "    print(\"Current Letter:\", letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "age_new\ngender_new\nheight_new\n"
     ]
    }
   ],
   "source": [
    "columnnames = [\"age\", \"gender\", \"height\"]\n",
    "def column_name_changer(names):\n",
    "    for i in names:\n",
    "        print(i+\"_new\")\n",
    "\n",
    "column_name_changer(columnnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index(['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch',\n       'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "import pandas as pd\n",
    "path = \"/Users/largo/Library/Mobile Documents/com~apple~CloudDocs/DataScience/Case_Studies/Titanic_ML_Disaster/test.csv\"\n",
    "titanic = pd.read_csv(path, sep=\",\", header=0)\n",
    "#data.columns = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "print(titanic.columns)\n",
    "titanic.head()\n",
    "\n",
    "# Renaming Function\n",
    "def column_renamer(dataframe):\n",
    "    names = dataframe.columns\n",
    "    new = []\n",
    "    for i in names:\n",
    "        new.append(i + \"_new\")\n",
    "    return new\n",
    "        \n",
    "#titanic.columns = column_renamer(titanic)\n",
    "#titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'withColumnRenamed'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-5dd7ae3db7bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrename_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitanic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Bujaa\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-80-39032acc976f>\u001b[0m in \u001b[0;36mrename_column\u001b[0;34m(df, prefix)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mdf_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"__\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/pyspark_env/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5463\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5464\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5465\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'withColumnRenamed'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a Function that splits the data by \"Pclass_new\" and writes a separate file for each\n",
    "# Output the datasets into three datasets inside python: df1, df2, df3 = function(titanic)\n",
    "\"\"\"\n",
    "This function takes a dataframe and separates tit depending on a columns unique values\n",
    "\"\"\"\n",
    "from collections import namedtuple\n",
    "\n",
    "def separator(dataset):\n",
    "    file1 = []\n",
    "    file2 = []\n",
    "    file3 = []\n",
    "    file1 = dataset.loc[titanic.Pclass==1, :]\n",
    "    file2 = dataset.loc[titanic.Pclass==2, :]\n",
    "    file3 = dataset.loc[titanic.Pclass==3, :]\n",
    "    Desc = namedtuple(\"Desc\", [\"test1\", \"test2\", \"test3\"])\n",
    "    return Desc(\n",
    "        file1,\n",
    "        file2,\n",
    "        file3\n",
    "    )\n",
    "\n",
    "df1, df2, df3 = separator(titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'column'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-152ce7c21edb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mresult_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_separator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitanic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitanic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPclass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-79-152ce7c21edb>\u001b[0m in \u001b[0;36mcolumn_separator\u001b[0;34m(dataset, column)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcolumn_separator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdata_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0munique_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mdata_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/pyspark_env/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5463\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5464\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5465\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'column'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Identify the unique values inside a column\n",
    "use each to split the dataframe\n",
    "put the dataframes in a list?\n",
    "loop over that list and put them in separate dataframes\n",
    "Output should be two tings: list with df's AND list with the names od the df's\n",
    "\"\"\"\n",
    "def column_separator(dataset, column):\n",
    "    data_list = []\n",
    "    unique_values = set(dataset.column)\n",
    "    for i in unique_values:\n",
    "        data_list.append([dataset.loc[dataset.column==i, :]])\n",
    "    return data_list\n",
    "\n",
    "result_list = column_separator(titanic, titanic.Pclass)\n",
    "\n",
    "\n"
   ]
  }
 ]
}